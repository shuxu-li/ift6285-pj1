{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/gc/b51k01w57r154kbndyfqrbh00000gn/T/ipykernel_2449/935870634.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/pandas/core/api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/pandas/_libs/__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/pandas/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ArrowDtype,\n\u001b[1;32m     52\u001b[0m     Int8Dtype,\n\u001b[1;32m     53\u001b[0m     Int16Dtype,\n\u001b[1;32m     54\u001b[0m     Int32Dtype,\n\u001b[1;32m     55\u001b[0m     Int64Dtype,\n\u001b[1;32m     56\u001b[0m     UInt8Dtype,\n\u001b[1;32m     57\u001b[0m     UInt16Dtype,\n\u001b[1;32m     58\u001b[0m     UInt32Dtype,\n\u001b[1;32m     59\u001b[0m     UInt64Dtype,\n\u001b[1;32m     60\u001b[0m     Float32Dtype,\n\u001b[1;32m     61\u001b[0m     Float64Dtype,\n\u001b[1;32m     62\u001b[0m     CategoricalDtype,\n\u001b[1;32m     63\u001b[0m     PeriodDtype,\n\u001b[1;32m     64\u001b[0m     IntervalDtype,\n\u001b[1;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     66\u001b[0m     StringDtype,\n\u001b[1;32m     67\u001b[0m     BooleanDtype,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     NA,\n\u001b[1;32m     70\u001b[0m     isna,\n\u001b[1;32m     71\u001b[0m     isnull,\n\u001b[1;32m     72\u001b[0m     notna,\n\u001b[1;32m     73\u001b[0m     notnull,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     Index,\n\u001b[1;32m     76\u001b[0m     CategoricalIndex,\n\u001b[1;32m     77\u001b[0m     RangeIndex,\n\u001b[1;32m     78\u001b[0m     MultiIndex,\n\u001b[1;32m     79\u001b[0m     IntervalIndex,\n\u001b[1;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     81\u001b[0m     DatetimeIndex,\n\u001b[1;32m     82\u001b[0m     PeriodIndex,\n\u001b[1;32m     83\u001b[0m     IndexSlice,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     NaT,\n\u001b[1;32m     86\u001b[0m     Period,\n\u001b[1;32m     87\u001b[0m     period_range,\n\u001b[1;32m     88\u001b[0m     Timedelta,\n\u001b[1;32m     89\u001b[0m     timedelta_range,\n\u001b[1;32m     90\u001b[0m     Timestamp,\n\u001b[1;32m     91\u001b[0m     date_range,\n\u001b[1;32m     92\u001b[0m     bdate_range,\n\u001b[1;32m     93\u001b[0m     Interval,\n\u001b[1;32m     94\u001b[0m     interval_range,\n\u001b[1;32m     95\u001b[0m     DateOffset,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     to_numeric,\n\u001b[1;32m     98\u001b[0m     to_datetime,\n\u001b[1;32m     99\u001b[0m     to_timedelta,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     Flags,\n\u001b[1;32m    102\u001b[0m     Grouper,\n\u001b[1;32m    103\u001b[0m     factorize,\n\u001b[1;32m    104\u001b[0m     unique,\n\u001b[1;32m    105\u001b[0m     value_counts,\n\u001b[1;32m    106\u001b[0m     NamedAgg,\n\u001b[1;32m    107\u001b[0m     array,\n\u001b[1;32m    108\u001b[0m     Categorical,\n\u001b[1;32m    109\u001b[0m     set_eng_float_format,\n\u001b[1;32m    110\u001b[0m     Series,\n\u001b[1;32m    111\u001b[0m     DataFrame,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/nlp_cours/lib/python3.10/site-packages/pandas/_libs/__init__.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "# feature\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tâche 2: présire la longueur de l'article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = pd.read_csv('train-lishuxu-article_length-title+abstract.csv', sep=';', index_col='id', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "test_length = pd.read_csv('test-lishuxu-article_length-title+abstract.csv', sep=';',index_col='id', quotechar='\"', quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = train_length.dropna(how='any')\n",
    "test_length = test_length.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set NaN values:\\n\", train_length.isnull().sum())\n",
    "print(\"Test set NaN values:\\n\", test_length.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_length.shape)\n",
    "print(test_length.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diviser les features et les label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_length['title'] + train_length['abstract']\n",
    "y_train = train_length['label']\n",
    "X_test = test_length['title'] + test_length['abstract']\n",
    "y_test = test_length['label']\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définir les baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline1 count_lr\n",
    "count_lr = Pipeline([('count', CountVectorizer()),('lr', LogisticRegression())])\n",
    "# baseline2 tfidf_mnb\n",
    "tfidf_mnb = Pipeline([('tfidf', TfidfVectorizer()),('mnb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultat de baseline1\n",
    "count_lr.fit(X_train, y_train)\n",
    "y_pred_count_lr = count_lr.predict(X_test)\n",
    "\n",
    "prediction_count_lr = pd.DataFrame({'id': test_length.index, 'label': y_pred_count_lr})\n",
    "prediction_count_lr.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-count_lr.csv', index=False)\n",
    "\n",
    "print('count_lr accuracy:', accuracy_score(y_test, y_pred_count_lr))\n",
    "print(classification_report(y_test, y_pred_count_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparer aux autres modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modèle simples du sklearn\n",
    "tfidf_lr = Pipeline([('tfidf', TfidfVectorizer()),('lr', LogisticRegression())])\n",
    "tfidf_svm = Pipeline([('tfidf', TfidfVectorizer()),('svm', SVC())])\n",
    "count_mnb = Pipeline([('count', CountVectorizer()),('mnb', MultinomialNB())])\n",
    "count_svm = Pipeline([('count', CountVectorizer()),('svm', SVC())])\n",
    "count_rf = Pipeline([('count', CountVectorizer()),('rf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_lr\n",
    "tfidf_lr.fit(X_train, y_train)\n",
    "y_pred_tfidf_lr = tfidf_lr.predict(X_test)\n",
    "\n",
    "prediction_tfidf_lr = pd.DataFrame({'id': test_length.index, 'label': y_pred_tfidf_lr})\n",
    "prediction_tfidf_lr.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-tfidf_lr.csv', index=False)\n",
    "\n",
    "print('tfidf_lr accuracy:', accuracy_score(y_test, y_pred_tfidf_lr))\n",
    "print(classification_report(y_test, y_pred_tfidf_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# résultat de baseline2\n",
    "tfidf_mnb.fit(X_train, y_train)\n",
    "y_pred_tfidf_mnb = tfidf_mnb.predict(X_test)\n",
    "\n",
    "prediction_tfidf_mnb = pd.DataFrame({'id': test_length.index, 'label': y_pred_tfidf_mnb})\n",
    "prediction_tfidf_mnb.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-tfidf_mnb.csv', index=False)\n",
    "\n",
    "print('tfidf_mnb accuracy:', accuracy_score(y_test, y_pred_tfidf_mnb))\n",
    "print(classification_report(y_test, y_pred_tfidf_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_svm\n",
    "tfidf_svm.fit(X_train, y_train)\n",
    "y_pred_tfidf_svm = tfidf_svm.predict(X_test)\n",
    "\n",
    "prediction_tfidf_svm = pd.DataFrame({'id': test_length.index, 'label': y_pred_tfidf_svm})\n",
    "prediction_tfidf_svm.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-tfidf_svm.csv', index=False)\n",
    "\n",
    "print('tfidf_svm accuracy:', accuracy_score(y_test, y_pred_tfidf_svm))\n",
    "print(classification_report(y_test, y_pred_tfidf_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_rf\n",
    "tfidf_rf = Pipeline(\n",
    "    [('tfidf', TfidfVectorizer()),('rf', RandomForestClassifier())]\n",
    ")\n",
    "tfidf_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tfidf_rf = tfidf_rf.predict(X_test)\n",
    "\n",
    "prediction_tfidf_rf = pd.DataFrame({'id': test_length.index, 'label': y_pred_tfidf_rf})\n",
    "prediction_tfidf_rf.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-tfidf_rf.csv', index=False)\n",
    "\n",
    "print('tfidf_rf accuracy:', accuracy_score(y_test, y_pred_tfidf_rf))\n",
    "print(classification_report(y_test, y_pred_tfidf_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_mnb\n",
    "count_mnb.fit(X_train, y_train)\n",
    "y_pred_count_mnb = count_mnb.predict(X_test)\n",
    "\n",
    "prediction_count_mnb = pd.DataFrame({'id': test_length.index, 'label': y_pred_count_mnb})\n",
    "prediction_count_mnb.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-count_mnb.csv', index=False)\n",
    "\n",
    "print('count_mnb accuracy:', accuracy_score(y_test, y_pred_count_mnb))\n",
    "print(classification_report(y_test, y_pred_count_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_svm\n",
    "count_svm.fit(X_train, y_train)\n",
    "y_pred_count_svm = count_svm.predict(X_test)\n",
    "\n",
    "prediction_count_svm = pd.DataFrame({'id': test_length.index, 'label': y_pred_count_svm})\n",
    "prediction_count_svm.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-count_svm.csv', index=False)\n",
    "\n",
    "print('count_svm accuracy:', accuracy_score(y_test, y_pred_count_svm))\n",
    "print(classification_report(y_test, y_pred_count_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_rf\n",
    "count_rf.fit(X_train, y_train)\n",
    "y_pred_count_rf = count_rf.predict(X_test)\n",
    "\n",
    "prediction_count_rf = pd.DataFrame({'id': test_length.index, 'label': y_pred_count_rf})\n",
    "prediction_count_rf.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-count_rf.csv', index=False)\n",
    "\n",
    "print('count_rf accuracy:', accuracy_score(y_test, y_pred_count_rf))\n",
    "print(classification_report(y_test, y_pred_count_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modifier countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2gram_lr = Pipeline([('count', CountVectorizer(ngram_range=(2, 2))),('lr', LogisticRegression())])\n",
    "\n",
    "count_2gram_lr.fit(X_train, y_train)\n",
    "y_pred_count_2gram_lr = count_2gram_lr.predict(X_test)\n",
    "\n",
    "prediction_count_2gram_lr = pd.DataFrame({'id': test_length.index, 'label': y_pred_count_2gram_lr})\n",
    "prediction_count_2gram_lr.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-count_2gram_lr.csv', index=False)\n",
    "\n",
    "print('count_2gram_lr accuracy:', accuracy_score(y_test, y_pred_count_2gram_lr))\n",
    "print(classification_report(y_test, y_pred_count_2gram_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2gram_svm = Pipeline([('count', CountVectorizer(ngram_range=(2, 3))),('svm', SVC())])\n",
    "count_2gram_svm.fit(X_train, y_train)\n",
    "y_pred_count_2gram_svm = count_2gram_svm.predict(X_test)\n",
    "\n",
    "# prediction_count_2gram_svm = pd.DataFrame({'id': test_length.index, 'label': y_pred_count_2gram_svm})\n",
    "\n",
    "# prediction_count_2gram_svm.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-count_2gram_svm.csv', index=False)\n",
    "\n",
    "print('count_2gram_svm accuracy:', accuracy_score(y_test, y_pred_count_2gram_svm))\n",
    "print(classification_report(y_test, y_pred_count_2gram_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2gram_mnb = Pipeline([('count', CountVectorizer(ngram_range=(2, 2))),('mnb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essayer word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Word2Vec to represent text\n",
    "sentences = [text.split() for text in X_train]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# extract features from Word2Vec\n",
    "def vectorize_text(text):\n",
    "    words = text.split()\n",
    "    vector = np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv], axis=0)\n",
    "    return vector if vector is not None else np.zeros(100)\n",
    "\n",
    "X_train_word2vec = np.array([vectorize_text(text) for text in X_train])\n",
    "X_test_word2vec = np.array([vectorize_text(text) for text in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_lr = LogisticRegression(max_iter=1000)\n",
    "w2v_lr.fit(X_train_word2vec, y_train)\n",
    "\n",
    "y_pred_w2v_lr = w2v_lr.predict(X_test_word2vec)\n",
    "\n",
    "prediction_w2v_lr = pd.DataFrame({'id': test_length.index, 'label': y_pred_w2v_lr})\n",
    "prediction_w2v_lr.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-w2v_lr.csv', index=False)\n",
    "\n",
    "print('w2v_lr accuracy:', accuracy_score(y_test, y_pred_w2v_lr))\n",
    "print(classification_report(y_test, y_pred_w2v_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_svm = SVC()\n",
    "w2v_svm.fit(X_train_word2vec, y_train)\n",
    "\n",
    "y_pred_w2v_svm = w2v_svm.predict(X_test_word2vec)\n",
    "\n",
    "prediction_w2v_svm = pd.DataFrame({'id': test_length.index, 'label': y_pred_w2v_svm})\n",
    "prediction_w2v_svm.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-w2v_svm.csv', index=False)\n",
    "\n",
    "print('w2v_svm accuracy:', accuracy_score(y_test, y_pred_w2v_svm))\n",
    "print(classification_report(y_test, y_pred_w2v_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_rf = RandomForestClassifier()\n",
    "w2v_rf.fit(X_train_word2vec, y_train)\n",
    "\n",
    "y_pred_w2v_rf = w2v_rf.predict(X_test_word2vec)\n",
    "\n",
    "prediction_w2v_rf = pd.DataFrame({'id': test_length.index, 'label': y_pred_w2v_rf})\n",
    "prediction_w2v_rf.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-w2v_rf.csv', index=False)\n",
    "\n",
    "print('w2v_rf accuracy:', accuracy_score(y_test, y_pred_w2v_rf))\n",
    "print(classification_report(y_test, y_pred_w2v_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essayer bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings_batch(texts):\n",
    "    # ensure that the input is a list\n",
    "    texts = texts.tolist()\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # move tensor to cpu\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()\n",
    "\n",
    "# embedding\n",
    "X_train_bert = np.vstack([get_bert_embeddings_batch(X_train[i:i+32]) for i in range(0, len(X_train), 32)])\n",
    "X_test_bert = np.vstack([get_bert_embeddings_batch(X_test[i:i+32]) for i in range(0, len(X_test), 32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_svm = SVC()\n",
    "bert_svm.fit(X_train_bert, y_train)\n",
    "\n",
    "y_pred_bert_svm = bert_svm.predict(X_test_bert)\n",
    "\n",
    "prediction_bert_svm = pd.DataFrame({'id': test_length.index, 'label': y_pred_bert_svm})\n",
    "prediction_bert_svm.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-bert_svm.csv', index=False)\n",
    "\n",
    "print('bert_svm accuracy:', accuracy_score(y_test, y_pred_bert_svm))\n",
    "print(classification_report(y_test, y_pred_bert_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_svm_court = SVC()\n",
    "bert_svm_court.fit(X_train_bert[:6000], y_train[:6000])\n",
    "\n",
    "y_pred_bert_svm_court = bert_svm_court.predict(X_test_bert)\n",
    "\n",
    "\n",
    "\n",
    "print('bert_svm_court accuracy:', accuracy_score(y_test, y_pred_bert_svm_court))\n",
    "print(classification_report(y_test, y_pred_bert_svm_court))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_lr = LogisticRegression(max_iter=1000)\n",
    "bert_lr.fit(X_train_bert, y_train)\n",
    "\n",
    "y_pred_bert_lr = bert_lr.predict(X_test_bert)\n",
    "\n",
    "prediction_bert_lr = pd.DataFrame({'id': test_length.index, 'label': y_pred_bert_lr})\n",
    "prediction_bert_lr.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-bert_lr.csv', index=False)\n",
    "\n",
    "print('bert_lr accuracy:', accuracy_score(y_test, y_pred_bert_lr))\n",
    "print(classification_report(y_test, y_pred_bert_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_rf = RandomForestClassifier()\n",
    "bert_rf.fit(X_train_bert, y_train)\n",
    "\n",
    "y_pred_bert_rf = bert_rf.predict(X_test_bert)\n",
    "\n",
    "prediction_bert_rf = pd.DataFrame({'id': test_length.index, 'label': y_pred_bert_rf})\n",
    "prediction_bert_rf.to_csv('../lishuxu/sortie/lishuxu-article_length-title+abstract-bert_rf.csv', index=False)\n",
    "print('bert_rf accuracy:', accuracy_score(y_test, y_pred_bert_rf))\n",
    "print(classification_report(y_test, y_pred_bert_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_length = {\n",
    "    'Model': ['tfidf_mnb', 'tfidf_lr', 'tfidf_svm', 'tfidf_rf', 'count_lr', 'count_mnb', 'count_svm', 'count_rf', 'w2v_lr', 'w2v_svm','w2v_rf', 'bert_rf', 'bert_svm', 'bert_lr'],\n",
    "\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_tfidf_mnb),\n",
    "        accuracy_score(y_test, y_pred_tfidf_lr),\n",
    "        accuracy_score(y_test, y_pred_tfidf_svm),\n",
    "        accuracy_score(y_test, y_pred_tfidf_rf),\n",
    "        accuracy_score(y_test, y_pred_count_lr),\n",
    "        accuracy_score(y_test, y_pred_count_mnb),\n",
    "        accuracy_score(y_test, y_pred_count_svm),\n",
    "        accuracy_score(y_test, y_pred_count_rf),\n",
    "        accuracy_score(y_test, y_pred_w2v_lr),\n",
    "        accuracy_score(y_test, y_pred_w2v_svm),\n",
    "        accuracy_score(y_test, y_pred_w2v_rf),\n",
    "        accuracy_score(y_test, y_pred_bert_rf),\n",
    "        accuracy_score(y_test, y_pred_bert_svm),\n",
    "        accuracy_score(y_test, y_pred_bert_lr)\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_length = pd.DataFrame(results_length)\n",
    "results_length['Accuracy'] = results_length['Accuracy'].round(3)\n",
    "print(results_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scibert\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased', num_labels=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_cours",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
